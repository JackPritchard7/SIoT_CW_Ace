{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5712b7",
   "metadata": {},
   "source": [
    "# ACE Tennis Shot Classifier\n",
    "\n",
    "Two-stage neural network system for classifying tennis shots from wrist-worn IMU data.\n",
    "\n",
    "## Overview\n",
    "- **Stage A**: Binary classification (Idle vs Swing)\n",
    "- **Stage B**: Multiclass classification (Forehand vs Backhand vs Serve)\n",
    "\n",
    "## Requirements\n",
    "- TensorFlow/Keras\n",
    "- scikit-learn\n",
    "- pandas, numpy\n",
    "- matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfa7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9357c0",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these paths to match your data directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - update these to match your directory structure\n",
    "DATA_DIR = 'UPDATE_HERE'\n",
    "FOREHAND_CSV = f'{DATA_DIR}/Forehands_100Hz_500Batch_Wrist.csv'\n",
    "BACKHAND_CSV = f'{DATA_DIR}/Backhands_100Hz_500Batch_Wrist.csv'\n",
    "SERVE_CSV = f'{DATA_DIR}/Serves_100Hz_500Batch_Wrist.csv'\n",
    "IDLE_CSV = f'{DATA_DIR}/Idle_100Hz_500Batch_Wrist.csv'\n",
    "\n",
    "# Output directory for trained models\n",
    "ARTIFACT_DIR = '../ACE_Trained_NN'\n",
    "\n",
    "# Sampling configuration (100Hz IMU data)\n",
    "SAMPLE_HZ = 100.0\n",
    "WINDOW_SIZE = 150  # 1.5 seconds\n",
    "PRE_SAMPLES = 70   # 0.7s before peak\n",
    "POST_SAMPLES = 80  # 0.8s after peak\n",
    "\n",
    "# Detection thresholds\n",
    "MOTION_TRIGGER = 21.0               # Acceleration threshold for swing detection (m/s¬≤)\n",
    "IDLE_MAX = MOTION_TRIGGER - 1       # Max acceleration for idle classification\n",
    "IDLE_RATIO = 0.8                    # Proportion of samples below IDLE_MAX for idle window\n",
    "\n",
    "# Model settings\n",
    "RANDOM_STATE = 0\n",
    "CLASS_NAMES_A = [\"idle\", \"swing\"]\n",
    "CLASS_NAMES_B = [\"backhand\", \"forehand\", \"serve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59147e09",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_influx_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean InfluxDB CSV export.\"\"\"\n",
    "    df = pd.read_csv(path, skiprows=3)\n",
    "    df = df.iloc[:, 3:]  # Remove metadata columns\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"_time\"] = pd.to_datetime(df[\"_time\"], errors=\"coerce\")\n",
    "    df.dropna(subset=[\"_time\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36b531e",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Extracts 35 optimized features from each 1.5-second window:\n",
    "- **Statistical features (24)**: mean, std, max for 8 IMU channels\n",
    "- **Biomechanical features (6)**: wrist rotation, velocity, follow-through\n",
    "- **Temporal features (5)**: smoothness, energy, trajectory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eadd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(window_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Extract 35 optimized features from a 1.5s window.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Part 1: Statistical features (24)\n",
    "    channels = [\"Amag\", \"Ax\", \"Ay\", \"Az\", \"Gmag\", \"Gx\", \"Gy\", \"Gz\"]\n",
    "    for ch in channels:\n",
    "        if ch not in window_df.columns:\n",
    "            continue\n",
    "        data = window_df[ch].values\n",
    "        features.extend([np.mean(data), np.std(data), np.max(data)])\n",
    "    \n",
    "    # Part 2: Biomechanical features (6)\n",
    "    Ax, Ay, Az = window_df[\"Ax\"].values, window_df[\"Ay\"].values, window_df[\"Az\"].values\n",
    "    Gx, Gy, Gz = window_df[\"Gx\"].values, window_df[\"Gy\"].values, window_df[\"Gz\"].values\n",
    "    Amag, Gmag = window_df[\"Amag\"].values, window_df[\"Gmag\"].values\n",
    "    \n",
    "    pronation_ratio = np.sum(Gx > 0) / len(Gx)\n",
    "    wrist_flexion_mean = np.mean(np.abs(Gy))\n",
    "    forearm_rotation_vel = np.mean(np.abs(Gx))\n",
    "    \n",
    "    peak_idx = np.argmax(Amag)\n",
    "    follow_through = np.mean(Amag[peak_idx:]) if peak_idx < len(Amag) - 1 else Amag[-1]\n",
    "    \n",
    "    lateral_swing = np.mean(np.abs(Ax))\n",
    "    vertical_lift = np.mean(Az)\n",
    "    \n",
    "    features.extend([pronation_ratio, wrist_flexion_mean, forearm_rotation_vel, \n",
    "                     follow_through, lateral_swing, vertical_lift])\n",
    "    \n",
    "    # Part 3: Temporal analysis (5)\n",
    "    accel_jerk = np.mean(np.abs(np.diff(Amag)))\n",
    "    \n",
    "    accel_vectors = np.column_stack([Ax, Ay, Az])\n",
    "    direction_changes = []\n",
    "    for i in range(1, len(accel_vectors)):\n",
    "        v1, v2 = accel_vectors[i-1], accel_vectors[i]\n",
    "        norm1, norm2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "        if norm1 > 0 and norm2 > 0:\n",
    "            cos_angle = np.clip(np.dot(v1, v2) / (norm1 * norm2), -1.0, 1.0)\n",
    "            direction_changes.append(np.arccos(cos_angle))\n",
    "    trajectory_curvature = np.mean(direction_changes) if direction_changes else 0.0\n",
    "    \n",
    "    threshold = np.max(Amag) * 0.3\n",
    "    post_peak = Amag[peak_idx:]\n",
    "    follow_through_length = np.sum(post_peak >= threshold) / len(post_peak) if peak_idx < len(Amag) - 1 else 0.0\n",
    "    \n",
    "    middle_third = Amag[len(Amag)//3:2*len(Amag)//3]\n",
    "    energy_release = np.sum(middle_third) / (len(middle_third) + 1e-6)\n",
    "    \n",
    "    last_third = Amag[2*len(Amag)//3:]\n",
    "    recovery_phase = np.mean(last_third)\n",
    "    \n",
    "    features.extend([accel_jerk, trajectory_curvature, follow_through_length, \n",
    "                     energy_release, recovery_phase])\n",
    "    \n",
    "    return np.nan_to_num(np.array(features, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d95f8ba",
   "metadata": {},
   "source": [
    "## Window Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks_over_threshold(amag, threshold, neighborhood=2):\n",
    "    \"\"\"Detect peaks in acceleration magnitude.\"\"\"\n",
    "    peaks = []\n",
    "    for i in range(neighborhood, len(amag) - neighborhood):\n",
    "        if amag[i] <= threshold:\n",
    "            continue\n",
    "        local_max = np.max(amag[i-neighborhood:i+neighborhood+1])\n",
    "        if amag[i] == local_max:\n",
    "            peaks.append(i)\n",
    "    return np.array(peaks, dtype=int)\n",
    "\n",
    "\n",
    "def windows_from_swings(df: pd.DataFrame, label: str):\n",
    "    \"\"\"Extract windows around detected swing peaks.\"\"\"\n",
    "    am = df[\"Amag\"].values\n",
    "    peaks = find_peaks_over_threshold(am, MOTION_TRIGGER)\n",
    "    out = []\n",
    "    for p in peaks:\n",
    "        s, e = p - PRE_SAMPLES, p + POST_SAMPLES\n",
    "        if s < 0 or e > len(df):\n",
    "            continue\n",
    "        w = df.iloc[s:e]\n",
    "        if len(w) == WINDOW_SIZE:\n",
    "            out.append((extract_features(w), label))\n",
    "    return out\n",
    "\n",
    "\n",
    "def windows_from_idle(df: pd.DataFrame, label: str = \"idle\"):\n",
    "    \"\"\"Extract idle windows (low acceleration segments).\"\"\"\n",
    "    out = []\n",
    "    i, n = 0, len(df)\n",
    "    while i + WINDOW_SIZE <= n:\n",
    "        seg = df.iloc[i:i+WINDOW_SIZE]\n",
    "        below = np.mean(seg[\"Amag\"].values < IDLE_MAX)\n",
    "        if below >= IDLE_RATIO:\n",
    "            out.append((extract_features(seg), label))\n",
    "            i += WINDOW_SIZE\n",
    "        else:\n",
    "            i += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a08f5",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets():\n",
    "    \"\"\"Load data and create datasets for both stages.\"\"\"\n",
    "    fh = load_influx_csv(FOREHAND_CSV)\n",
    "    bh = load_influx_csv(BACKHAND_CSV)\n",
    "    sv = load_influx_csv(SERVE_CSV)\n",
    "    idf = load_influx_csv(IDLE_CSV)\n",
    "    \n",
    "    swing_rows = []\n",
    "    swing_rows += windows_from_swings(fh, \"forehand\")\n",
    "    swing_rows += windows_from_swings(bh, \"backhand\")\n",
    "    swing_rows += windows_from_swings(sv, \"serve\")\n",
    "    idle_rows = windows_from_idle(idf, \"idle\")\n",
    "    \n",
    "    # Stage A: Idle vs Swing\n",
    "    X_A, y_A = [], []\n",
    "    for feat, _ in idle_rows:\n",
    "        X_A.append(feat)\n",
    "        y_A.append(\"idle\")\n",
    "    for feat, _ in swing_rows:\n",
    "        X_A.append(feat)\n",
    "        y_A.append(\"swing\")\n",
    "    \n",
    "    # Stage B: Stroke classification\n",
    "    X_B = [f for f, l in swing_rows]\n",
    "    y_B = [l for f, l in swing_rows]\n",
    "    \n",
    "    return (np.vstack(X_A), np.array(y_A)), (np.vstack(X_B), np.array(y_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a768ac",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, num_classes):\n",
    "    \"\"\"Simple neural network with one hidden layer.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(32, activation='relu', input_shape=(input_size,)),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847a04d",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train both classifier stages with proper train/test splitting and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare datasets\n",
    "print(\"Loading data...\")\n",
    "(X_A, y_A), (X_B, y_B) = make_datasets()\n",
    "\n",
    "print(f\"Stage A distribution: {Counter(y_A.tolist())}\")\n",
    "print(f\"Stage B distribution: {Counter(y_B.tolist())}\")\n",
    "\n",
    "# ===============================================================\n",
    "# SHARED SCALER: Fit on Stage A training data\n",
    "# ===============================================================\n",
    "print(\"\\n‚öôÔ∏è  Fitting shared scaler on Stage A training data...\")\n",
    "print(\"   This scaler will be used for BOTH Stage A and Stage B\")\n",
    "print(\"   to ensure consistent normalization across all models.\\n\")\n",
    "\n",
    "label_map_A = {\"idle\": 0, \"swing\": 1}\n",
    "y_A_num = np.array([label_map_A[y] for y in y_A])\n",
    "\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(\n",
    "    X_A, y_A_num, test_size=0.2, random_state=RANDOM_STATE, stratify=y_A_num\n",
    ")\n",
    "\n",
    "# Fit ONE shared scaler on Stage A training data (includes idle + swing)\n",
    "shared_scaler = StandardScaler().fit(X_train_A)\n",
    "\n",
    "# ===============================================================\n",
    "# Stage A: Idle vs Swing\n",
    "# ===============================================================\n",
    "print(\"üéæ Stage A: Idle vs Swing\")\n",
    "\n",
    "# Apply shared scaler to Stage A data\n",
    "X_train_A_scaled = shared_scaler.transform(X_train_A)\n",
    "X_test_A_scaled = shared_scaler.transform(X_test_A)\n",
    "\n",
    "modelA = build_model(X_A.shape[1], len(label_map_A))\n",
    "\n",
    "# Add early stopping to stop training when validation loss stops improving\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training Stage A...\")\n",
    "historyA = modelA.fit(\n",
    "    X_train_A_scaled, y_train_A,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_A = np.argmax(modelA.predict(X_test_A_scaled), axis=1)\n",
    "accA = accuracy_score(y_test_A, y_pred_A)\n",
    "print(f\"\\n‚úÖ Stage A Accuracy: {accA:.3f}\")\n",
    "print(classification_report(y_test_A, y_pred_A, target_names=list(label_map_A.keys())))\n",
    "\n",
    "# Detailed Metrics Table for Stage A\n",
    "precision_A, recall_A, f1_A, support_A = precision_recall_fscore_support(y_test_A, y_pred_A)\n",
    "metrics_df_A = pd.DataFrame({\n",
    "    'Class': list(label_map_A.keys()),\n",
    "    'Precision': [f\"{p:.4f}\" for p in precision_A],\n",
    "    'Recall': [f\"{r:.4f}\" for r in recall_A],\n",
    "    'F1-Score': [f\"{f:.4f}\" for f in f1_A],\n",
    "    'Support': support_A\n",
    "})\n",
    "print(\"\\nüìä Stage A Detailed Metrics:\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_df_A.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cmA = confusion_matrix(y_test_A, y_pred_A)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(cmA, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=list(label_map_A.keys()),\n",
    "            yticklabels=list(label_map_A.keys()))\n",
    "plt.title(f\"Stage A Confusion Matrix (Acc={accA:.2f})\", color=\"darkorange\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy/loss curves\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(historyA.history[\"accuracy\"], label=\"Train Accuracy\", color=\"darkorange\")\n",
    "plt.plot(historyA.history[\"val_accuracy\"], label=\"Val Accuracy\", color=\"black\")\n",
    "plt.title(\"Stage A Training Progress\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Stage B: Stroke Type (Forehand / Backhand / Serve)\n",
    "# ===============================================================\n",
    "print(\"\\nüè∏ Stage B: Forehand / Backhand / Serve\")\n",
    "\n",
    "label_map_B = {\"backhand\": 0, \"forehand\": 1, \"serve\": 2}\n",
    "y_B_num = np.array([label_map_B[y] for y in y_B])\n",
    "\n",
    "# Split Stage B data\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(\n",
    "    X_B, y_B_num, test_size=0.2, random_state=RANDOM_STATE, stratify=y_B_num\n",
    ")\n",
    "\n",
    "# Apply SAME shared scaler to Stage B data\n",
    "X_train_B_scaled = shared_scaler.transform(X_train_B)\n",
    "X_test_B_scaled = shared_scaler.transform(X_test_B)\n",
    "\n",
    "print(\"‚úÖ Using shared scaler for Stage B (same as Stage A)\")\n",
    "\n",
    "modelB = build_model(X_B.shape[1], len(label_map_B))\n",
    "\n",
    "early_stop_B = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training Stage B...\")\n",
    "historyB = modelB.fit(\n",
    "    X_train_B_scaled, y_train_B,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop_B],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_B = np.argmax(modelB.predict(X_test_B_scaled), axis=1)\n",
    "accB = accuracy_score(y_test_B, y_pred_B)\n",
    "print(f\"\\n‚úÖ Stage B Accuracy: {accB:.3f}\")\n",
    "print(classification_report(y_test_B, y_pred_B, target_names=list(label_map_B.keys())))\n",
    "\n",
    "# Detailed Metrics Table for Stage B\n",
    "precision_B, recall_B, f1_B, support_B = precision_recall_fscore_support(y_test_B, y_pred_B)\n",
    "metrics_df_B = pd.DataFrame({\n",
    "    'Class': list(label_map_B.keys()),\n",
    "    'Precision': [f\"{p:.4f}\" for p in precision_B],\n",
    "    'Recall': [f\"{r:.4f}\" for r in recall_B],\n",
    "    'F1-Score': [f\"{f:.4f}\" for f in f1_B],\n",
    "    'Support': support_B\n",
    "})\n",
    "print(\"\\nüìä Stage B Detailed Metrics:\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_df_B.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cmB = confusion_matrix(y_test_B, y_pred_B)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cmB, annot=True, fmt=\"d\", cmap=\"Oranges\",\n",
    "            xticklabels=list(label_map_B.keys()),\n",
    "            yticklabels=list(label_map_B.keys()))\n",
    "plt.title(f\"Stage B Confusion Matrix (Acc={accB:.2f})\", color=\"darkorange\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy/loss curves\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(historyB.history[\"accuracy\"], label=\"Train Accuracy\", color=\"darkorange\")\n",
    "plt.plot(historyB.history[\"val_accuracy\"], label=\"Val Accuracy\", color=\"black\")\n",
    "plt.title(\"Stage B Training Progress\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae275e",
   "metadata": {},
   "source": [
    "## Save Models and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Path(ARTIFACT_DIR)\n",
    "out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "modelA.save(out / \"idle_swing_model.keras\")\n",
    "modelB.save(out / \"stroke_type_model.keras\")\n",
    "joblib.dump(shared_scaler, out / \"scaler.pkl\")\n",
    "(out / \"class_names_A.json\").write_text(json.dumps(CLASS_NAMES_A))\n",
    "(out / \"class_names_B.json\").write_text(json.dumps(CLASS_NAMES_B))\n",
    "\n",
    "print(f\"\\n‚úÖ Saved all artifacts to {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b622500",
   "metadata": {},
   "source": [
    "## Export Normalization Parameters for ESP32\n",
    "\n",
    "These parameters are used for feature normalization on the embedded device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Export Normalization Parameters for ESP32 and paste into arduino code\n",
    "# ===============================================================\n",
    "means = shared_scaler.mean_.astype(float)\n",
    "scales = np.sqrt(shared_scaler.var_).astype(float)\n",
    "\n",
    "print(\"\\n=== SHARED SCALER PARAMETERS (for ESP32) ===\")\n",
    "print(\"PASTE THESE INTO ESP32 CODE\")\n",
    "print(\"\\n=== FEATURE MEANS ===\")\n",
    "print(\", \".join([f\"{m:.6f}f\" for m in means]))\n",
    "print('length = ', len(means))\n",
    "\n",
    "print(\"\\n=== FEATURE SCALES ===\")\n",
    "print(\", \".join([f\"{s:.6f}f\" for s in scales]))\n",
    "print('length = ', len(scales))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad150012",
   "metadata": {},
   "source": [
    "## Convert Models to TensorFlow Lite for ESP32\n",
    "\n",
    "To deploy these models on an ESP32 microcontroller, convert them to TensorFlow Lite format:\n",
    "\n",
    "```bash\n",
    "# Convert both models to TensorFlow Lite (.tflite)\n",
    "python export_tflite.py\n",
    "\n",
    "\n",
    "# Convert TFLite models to C header files for Arduino\n",
    "python convert_to_header.py\n",
    "```\n",
    "\n",
    "This will generate:\n",
    "- `idle_swing_model.tflite` and `stroke_type_model.tflite`\n",
    "- Header files that can be included in your ESP32 firmware"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
